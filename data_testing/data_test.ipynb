{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "df1 = pd.read_excel(\"datasets/imbd_test_data.xlsx\")\n",
    "df2 = pd.read_excel(\"datasets/rotten_tomatoes.xlsx\")\n",
    "df3 = pd.read_excel(\"datasets/synthetic_100.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.head()\n",
    "# df2.head()\n",
    "# df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming some columns\n",
    "df1 = df1.rename({\"movie_name\":\"title\", \"plot\":\"synopsis\"}, axis=1)\n",
    "df2 = df2.rename({\"Title\":\"title\", \"Genre\":\"label\"}, axis=1)\n",
    "df3 = df3.rename({\"Title\":\"title\", \"Genre\":\"label\", \"Synopsis\":\"synopsis\"}, axis=1)\n",
    "\n",
    "# df1 - imbd dataset\n",
    "# df2 - rotten tomatoes dataset\n",
    "# df3 - synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_words_synopsis(data_sets):\n",
    "    new_datasets = []\n",
    "    for data in data_sets:\n",
    "        data[\"synopsis\"] = data[\"synopsis\"].str.lower()\n",
    "        new_datasets.append(data)\n",
    "\n",
    "    return new_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2, df3 = lower_words_synopsis(data_sets=[df1, df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Function for testing multiple datasets\n",
    "def data_tester(data_sets, models, report_path, acc=True, creport=True, \n",
    "                cfmat=True, tsize=0.20, rstate=45, vec=cvec):\n",
    "    for ind, data in enumerate(data_sets):\n",
    "        X, y = data[\"synopsis\"], data[\"label\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tsize, \n",
    "                                   random_state=rstate)\n",
    "        print(f\"\\n\\n--------> DATASET NUMBER - {ind + 1}\\n\\n\")\n",
    "        \n",
    "        try:\n",
    "            with open(report_path, \"a\") as file:\n",
    "                file.write(f\"--------> DATASET NUMBER - {ind + 1}\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Some exception occured: {e}\")\n",
    "            \n",
    "        for model in models:\n",
    "            pipe = Pipeline([\n",
    "                (\"cvec\", vec), \n",
    "                (\"model\", model)\n",
    "            ])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            preds = pipe.predict(X_test)\n",
    "\n",
    "            print(f\"Model name - {model}\")\n",
    "            if acc == True:\n",
    "                acc_score = accuracy_score(y_true=y_test, y_pred=preds)\n",
    "                print(f\"Overall Accuracy: {acc_score}\")\n",
    "            if creport == True:\n",
    "                print(\"\\nClassification report:\")\n",
    "                clf = classification_report(y_true=y_test, y_pred=preds)\n",
    "                print(clf)\n",
    "            if cfmat == True:\n",
    "                conf_mat = confusion_matrix(y_true=y_test, y_pred=preds)\n",
    "                print(\"\\nConfusion matrix:\")\n",
    "                print(conf_mat)\n",
    "                \n",
    "            try:\n",
    "                with open(report_path, \"a\") as file:\n",
    "                    file.write(f\"Model name: {model}\\n\")\n",
    "                    file.write(clf)\n",
    "                    file.write(\"\\n\")\n",
    "                    file.write(\"\\nConfusion matrix:\\n\")\n",
    "                    file.write(str(conf_mat) + \"\\n\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Some exception occured: {e}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------> DATASET NUMBER - 1\n",
      "\n",
      "\n",
      "Model name - GradientBoostingClassifier()\n",
      "Overall Accuracy: 0.675\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.68      0.65      0.67        20\n",
      "    thriller       0.67      0.70      0.68        20\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.68      0.68      0.67        40\n",
      "weighted avg       0.68      0.68      0.67        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  7]\n",
      " [ 6 14]]\n",
      "Model name - AdaBoostClassifier(algorithm='SAMME')\n",
      "Overall Accuracy: 0.7\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.72      0.65      0.68        20\n",
      "    thriller       0.68      0.75      0.71        20\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.70      0.70      0.70        40\n",
      "weighted avg       0.70      0.70      0.70        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  7]\n",
      " [ 5 15]]\n",
      "Model name - MultinomialNB()\n",
      "Overall Accuracy: 0.775\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.72      0.90      0.80        20\n",
      "    thriller       0.87      0.65      0.74        20\n",
      "\n",
      "    accuracy                           0.78        40\n",
      "   macro avg       0.79      0.78      0.77        40\n",
      "weighted avg       0.79      0.78      0.77        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[18  2]\n",
      " [ 7 13]]\n",
      "Model name - RandomForestClassifier()\n",
      "Overall Accuracy: 0.725\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.71      0.75      0.73        20\n",
      "    thriller       0.74      0.70      0.72        20\n",
      "\n",
      "    accuracy                           0.72        40\n",
      "   macro avg       0.73      0.72      0.72        40\n",
      "weighted avg       0.73      0.72      0.72        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[15  5]\n",
      " [ 6 14]]\n",
      "Model name - LogisticRegression()\n",
      "Overall Accuracy: 0.775\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.74      0.85      0.79        20\n",
      "    thriller       0.82      0.70      0.76        20\n",
      "\n",
      "    accuracy                           0.78        40\n",
      "   macro avg       0.78      0.77      0.77        40\n",
      "weighted avg       0.78      0.78      0.77        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[17  3]\n",
      " [ 6 14]]\n",
      "Model name - DecisionTreeClassifier()\n",
      "Overall Accuracy: 0.625\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.63      0.60      0.62        20\n",
      "    thriller       0.62      0.65      0.63        20\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.63      0.62      0.62        40\n",
      "weighted avg       0.63      0.62      0.62        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[12  8]\n",
      " [ 7 13]]\n",
      "\n",
      "\n",
      "--------> DATASET NUMBER - 2\n",
      "\n",
      "\n",
      "Model name - GradientBoostingClassifier()\n",
      "Overall Accuracy: 0.75\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.86      0.60      0.71        20\n",
      "    thriller       0.69      0.90      0.78        20\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.77      0.75      0.74        40\n",
      "weighted avg       0.77      0.75      0.74        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[12  8]\n",
      " [ 2 18]]\n",
      "Model name - AdaBoostClassifier(algorithm='SAMME')\n",
      "Overall Accuracy: 0.8\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.93      0.65      0.76        20\n",
      "    thriller       0.73      0.95      0.83        20\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.83      0.80      0.80        40\n",
      "weighted avg       0.83      0.80      0.80        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  7]\n",
      " [ 1 19]]\n",
      "Model name - MultinomialNB()\n",
      "Overall Accuracy: 0.95\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.95      0.95      0.95        20\n",
      "    thriller       0.95      0.95      0.95        20\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[19  1]\n",
      " [ 1 19]]\n",
      "Model name - RandomForestClassifier()\n",
      "Overall Accuracy: 0.875\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.89      0.85      0.87        20\n",
      "    thriller       0.86      0.90      0.88        20\n",
      "\n",
      "    accuracy                           0.88        40\n",
      "   macro avg       0.88      0.88      0.87        40\n",
      "weighted avg       0.88      0.88      0.87        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[17  3]\n",
      " [ 2 18]]\n",
      "Model name - LogisticRegression()\n",
      "Overall Accuracy: 0.925\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.90      0.95      0.93        20\n",
      "    thriller       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[19  1]\n",
      " [ 2 18]]\n",
      "Model name - DecisionTreeClassifier()\n",
      "Overall Accuracy: 0.7\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.83      0.50      0.62        20\n",
      "    thriller       0.64      0.90      0.75        20\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.74      0.70      0.69        40\n",
      "weighted avg       0.74      0.70      0.69        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[10 10]\n",
      " [ 2 18]]\n",
      "\n",
      "\n",
      "--------> DATASET NUMBER - 3\n",
      "\n",
      "\n",
      "Model name - GradientBoostingClassifier()\n",
      "Overall Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Romantic       1.00      1.00      1.00        20\n",
      "    Thriller       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [ 0 20]]\n",
      "Model name - AdaBoostClassifier(algorithm='SAMME')\n",
      "Overall Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Romantic       1.00      1.00      1.00        20\n",
      "    Thriller       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [ 0 20]]\n",
      "Model name - MultinomialNB()\n",
      "Overall Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Romantic       1.00      1.00      1.00        20\n",
      "    Thriller       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [ 0 20]]\n",
      "Model name - RandomForestClassifier()\n",
      "Overall Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Romantic       1.00      1.00      1.00        20\n",
      "    Thriller       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [ 0 20]]\n",
      "Model name - LogisticRegression()\n",
      "Overall Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Romantic       1.00      1.00      1.00        20\n",
      "    Thriller       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [ 0 20]]\n",
      "Model name - DecisionTreeClassifier()\n",
      "Overall Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Romantic       1.00      1.00      1.00        20\n",
      "    Thriller       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [ 0 20]]\n"
     ]
    }
   ],
   "source": [
    "data_tester(\n",
    "    data_sets=[df1, df2, df3],\n",
    "    models=[\n",
    "        GradientBoostingClassifier(),\n",
    "        AdaBoostClassifier(algorithm=\"SAMME\"),\n",
    "        MultinomialNB(),\n",
    "        RandomForestClassifier(),\n",
    "        LogisticRegression(), \n",
    "        DecisionTreeClassifier()\n",
    "    ],\n",
    "    report_path=\"./reports/report1.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion: \n",
    "The synthetic data (generated by gpt) is of very low quality. It is so low quality data that all the classification algorithms with their default hyper parameters overfits. So we won't be relying on synthetic data at all.\n",
    "\n",
    "**Let's generate another report.** In this report we will have the first two datasets, from imbd, rotten tomatoes, and third will be imbd and rotten tomatoes combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat(objs=[df1,df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------> DATASET NUMBER - 1\n",
      "\n",
      "\n",
      "Model name - GradientBoostingClassifier()\n",
      "Overall Accuracy: 0.7\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.72      0.65      0.68        20\n",
      "    thriller       0.68      0.75      0.71        20\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.70      0.70      0.70        40\n",
      "weighted avg       0.70      0.70      0.70        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  7]\n",
      " [ 5 15]]\n",
      "Model name - AdaBoostClassifier(algorithm='SAMME')\n",
      "Overall Accuracy: 0.7\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.72      0.65      0.68        20\n",
      "    thriller       0.68      0.75      0.71        20\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.70      0.70      0.70        40\n",
      "weighted avg       0.70      0.70      0.70        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  7]\n",
      " [ 5 15]]\n",
      "Model name - MultinomialNB()\n",
      "Overall Accuracy: 0.775\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.72      0.90      0.80        20\n",
      "    thriller       0.87      0.65      0.74        20\n",
      "\n",
      "    accuracy                           0.78        40\n",
      "   macro avg       0.79      0.78      0.77        40\n",
      "weighted avg       0.79      0.78      0.77        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[18  2]\n",
      " [ 7 13]]\n",
      "Model name - RandomForestClassifier()\n",
      "Overall Accuracy: 0.7\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.70      0.70      0.70        20\n",
      "    thriller       0.70      0.70      0.70        20\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.70      0.70      0.70        40\n",
      "weighted avg       0.70      0.70      0.70        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[14  6]\n",
      " [ 6 14]]\n",
      "Model name - LogisticRegression()\n",
      "Overall Accuracy: 0.775\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.74      0.85      0.79        20\n",
      "    thriller       0.82      0.70      0.76        20\n",
      "\n",
      "    accuracy                           0.78        40\n",
      "   macro avg       0.78      0.77      0.77        40\n",
      "weighted avg       0.78      0.78      0.77        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[17  3]\n",
      " [ 6 14]]\n",
      "Model name - DecisionTreeClassifier()\n",
      "Overall Accuracy: 0.7\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.70      0.70      0.70        20\n",
      "    thriller       0.70      0.70      0.70        20\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.70      0.70      0.70        40\n",
      "weighted avg       0.70      0.70      0.70        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[14  6]\n",
      " [ 6 14]]\n",
      "\n",
      "\n",
      "--------> DATASET NUMBER - 2\n",
      "\n",
      "\n",
      "Model name - GradientBoostingClassifier()\n",
      "Overall Accuracy: 0.725\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.85      0.55      0.67        20\n",
      "    thriller       0.67      0.90      0.77        20\n",
      "\n",
      "    accuracy                           0.72        40\n",
      "   macro avg       0.76      0.73      0.72        40\n",
      "weighted avg       0.76      0.72      0.72        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[11  9]\n",
      " [ 2 18]]\n",
      "Model name - AdaBoostClassifier(algorithm='SAMME')\n",
      "Overall Accuracy: 0.75\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.78      0.70      0.74        20\n",
      "    thriller       0.73      0.80      0.76        20\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.75      0.75      0.75        40\n",
      "weighted avg       0.75      0.75      0.75        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[14  6]\n",
      " [ 4 16]]\n",
      "Model name - MultinomialNB()\n",
      "Overall Accuracy: 0.95\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.95      0.95      0.95        20\n",
      "    thriller       0.95      0.95      0.95        20\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[19  1]\n",
      " [ 1 19]]\n",
      "Model name - RandomForestClassifier()\n",
      "Overall Accuracy: 0.825\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.78      0.90      0.84        20\n",
      "    thriller       0.88      0.75      0.81        20\n",
      "\n",
      "    accuracy                           0.82        40\n",
      "   macro avg       0.83      0.82      0.82        40\n",
      "weighted avg       0.83      0.82      0.82        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[18  2]\n",
      " [ 5 15]]\n",
      "Model name - LogisticRegression()\n",
      "Overall Accuracy: 0.925\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.90      0.95      0.93        20\n",
      "    thriller       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[19  1]\n",
      " [ 2 18]]\n",
      "Model name - DecisionTreeClassifier()\n",
      "Overall Accuracy: 0.75\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.86      0.60      0.71        20\n",
      "    thriller       0.69      0.90      0.78        20\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.77      0.75      0.74        40\n",
      "weighted avg       0.77      0.75      0.74        40\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[12  8]\n",
      " [ 2 18]]\n",
      "\n",
      "\n",
      "--------> DATASET NUMBER - 3\n",
      "\n",
      "\n",
      "Model name - GradientBoostingClassifier()\n",
      "Overall Accuracy: 0.6375\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.62      0.64      0.63        39\n",
      "    thriller       0.65      0.63      0.64        41\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.64      0.64      0.64        80\n",
      "weighted avg       0.64      0.64      0.64        80\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[25 14]\n",
      " [15 26]]\n",
      "Model name - AdaBoostClassifier(algorithm='SAMME')\n",
      "Overall Accuracy: 0.7125\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.77      0.59      0.67        39\n",
      "    thriller       0.68      0.83      0.75        41\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.72      0.71      0.71        80\n",
      "weighted avg       0.72      0.71      0.71        80\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[23 16]\n",
      " [ 7 34]]\n",
      "Model name - MultinomialNB()\n",
      "Overall Accuracy: 0.8375\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.81      0.87      0.84        39\n",
      "    thriller       0.87      0.80      0.84        41\n",
      "\n",
      "    accuracy                           0.84        80\n",
      "   macro avg       0.84      0.84      0.84        80\n",
      "weighted avg       0.84      0.84      0.84        80\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[34  5]\n",
      " [ 8 33]]\n",
      "Model name - RandomForestClassifier()\n",
      "Overall Accuracy: 0.7375\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.74      0.72      0.73        39\n",
      "    thriller       0.74      0.76      0.75        41\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.74      0.74      0.74        80\n",
      "weighted avg       0.74      0.74      0.74        80\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[28 11]\n",
      " [10 31]]\n",
      "Model name - LogisticRegression()\n",
      "Overall Accuracy: 0.7875\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.81      0.74      0.77        39\n",
      "    thriller       0.77      0.83      0.80        41\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.79      0.79      0.79        80\n",
      "weighted avg       0.79      0.79      0.79        80\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[29 10]\n",
      " [ 7 34]]\n",
      "Model name - DecisionTreeClassifier()\n",
      "Overall Accuracy: 0.675\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    romantic       0.66      0.69      0.68        39\n",
      "    thriller       0.69      0.66      0.68        41\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.68      0.68      0.68        80\n",
      "weighted avg       0.68      0.68      0.68        80\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[27 12]\n",
      " [14 27]]\n"
     ]
    }
   ],
   "source": [
    "data_tester(\n",
    "    data_sets=[df1, df2, df3],\n",
    "    models=[\n",
    "        GradientBoostingClassifier(),\n",
    "        AdaBoostClassifier(algorithm=\"SAMME\"),\n",
    "        MultinomialNB(),\n",
    "        RandomForestClassifier(),\n",
    "        LogisticRegression(), \n",
    "        DecisionTreeClassifier()\n",
    "    ],\n",
    "    report_path=\"./reports/report2.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Conclusion:\n",
    "Even after combining both the datasets, the results were not that good. The rotten tomatoes dataset performs well as compared to the combined dataset. This is because the imbd dataset has a lower quality and it's causing a negative impact to the overall combined dataset. \n",
    "\n",
    "**So, we will proceed with the rotten tomatoes dataset.**  \n",
    "Because here we are getting the best results when we use logistic regression algorithm on DATASET NUMBER - 2, that is of 'Rotten Tomatoes'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
